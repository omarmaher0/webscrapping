{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu-mEu3BMi2b",
        "outputId": "0b267d20-2c11-4891-a0e0-2b42fd990020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed. Data saved to 'wuzzuf_jobs.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URL\n",
        "base_url = 'https://wuzzuf.net/search/jobs/?a=navbl%7Cspbl&filters%5Bcountry%5D%5B0%5D=Egypt&filters%5Bworkplace_arrangement%5D%5B0%5D=On-site&q=data%20analyst'\n",
        "\n",
        "# Initialize a list to store the job data\n",
        "job_data = []\n",
        "\n",
        "# Define the number of pages you want to scrape\n",
        "num_pages = 11  # Adjust this number based on how many pages you want to scrape\n",
        "\n",
        "for page in range(num_pages):\n",
        "    # Construct the URL for the current page\n",
        "    url = base_url + str(page)\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve data from page {page}\")\n",
        "        continue\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract job postings (you'll need to update the selectors based on the website's structure)\n",
        "    job_cards = soup.find_all('div', class_='css-1gatmva e1v1l3u10')  # Example selector; adjust as needed\n",
        "\n",
        "    for job in job_cards:\n",
        "        job_title = job.find('h2', class_='css-m604qf').text.strip()\n",
        "        company_name = job.find('a', class_='css-17s97q8').text.strip()\n",
        "        location = job.find('span', class_='css-5wys0k').text.strip()\n",
        "\n",
        "        # Add the job data to the list\n",
        "        job_data.append({\n",
        "            'Job Title': job_title,\n",
        "            'Company Name': company_name,\n",
        "            'Location': location,\n",
        "        })\n",
        "\n",
        "# Convert the job data to a pandas DataFrame\n",
        "df = pd.DataFrame(job_data)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df.to_csv('wuzzuf_jobs.csv', index=False)\n",
        "\n",
        "print(\"Scraping completed. Data saved to 'wuzzuf_jobs.csv'.\")\n"
      ]
    }
  ]
}